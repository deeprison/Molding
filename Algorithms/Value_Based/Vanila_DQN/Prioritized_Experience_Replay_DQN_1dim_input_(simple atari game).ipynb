{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of DQN paper for 1-dimensional games, such as Cartpole.\n",
    "- https://www.nature.com/articles/nature14236\n",
    "- https://arxiv.org/pdf/1312.5602.pdf\n",
    "\n",
    "<br>\n",
    "\n",
    "    Detailed implementation of Q-Network, a state and ReplayBuffer are different from the original paper. Because this notebook aims to solve a \"simple 1-dimensional\" atari game.\n",
    "    Please see the notebook named as \"..._2dim\" for more rigorous implementation of the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please NOTE,\n",
    "    The code lines different from Vanila DQN are annotated with '*/*/*/'.\n",
    "    So, by searching '*/*/*/', you can find these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    ''' Simple linear Q-Network. The architecture is, therefore, different from thg model in DQN paper.'''\n",
    "    def __init__(self, \n",
    "                 input_feature: (\"int: input state dimension\"), \n",
    "                 action_dim: (\"output: action dimensions\"),\n",
    "        ):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.linear1 = nn.Linear(input_feature, 256)\n",
    "        self.linear2 = nn.Linear(256, 128) \n",
    "        self.linear3 = nn.Linear(128, action_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear3(self.relu(self.linear2(x)))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# */*/*/\n",
    "# The following SegmentTree Classes come from OpenAI Source\n",
    "import operator\n",
    "\n",
    "class SegmentTree(object):\n",
    "    def __init__(self, capacity, operation, neutral_element):\n",
    "        \"\"\"Build a Segment Tree data structure.\n",
    "        https://en.wikipedia.org/wiki/Segment_tree\n",
    "        Can be used as regular array, but with two\n",
    "        important differences:\n",
    "            a) setting item's value is slightly slower.\n",
    "               It is O(lg capacity) instead of O(1).\n",
    "            b) user has access to an efficient ( O(log segment size) )\n",
    "               `reduce` operation which reduces `operation` over\n",
    "               a contiguous subsequence of items in the array.\n",
    "        Paramters\n",
    "        ---------\n",
    "        capacity: int\n",
    "            Total size of the array - must be a power of two.\n",
    "        operation: lambda obj, obj -> obj\n",
    "            and operation for combining elements (eg. sum, max)\n",
    "            must form a mathematical group together with the set of\n",
    "            possible values for array elements (i.e. be associative)\n",
    "        neutral_element: obj\n",
    "            neutral element for the operation above. eg. float('-inf')\n",
    "            for max and 0 for sum.\n",
    "        \"\"\"\n",
    "        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n",
    "        self._capacity = capacity\n",
    "        self._value = [neutral_element for _ in range(2 * capacity)]\n",
    "        self._operation = operation\n",
    "\n",
    "    def _reduce_helper(self, start, end, node, node_start, node_end):\n",
    "        if start == node_start and end == node_end:\n",
    "            return self._value[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self._operation(\n",
    "                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n",
    "                )\n",
    "\n",
    "    def reduce(self, start=0, end=None):\n",
    "        \"\"\"Returns result of applying `self.operation`\n",
    "        to a contiguous subsequence of the array.\n",
    "            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n",
    "        Parameters\n",
    "        ----------\n",
    "        start: int\n",
    "            beginning of the subsequence\n",
    "        end: int\n",
    "            end of the subsequences\n",
    "        Returns\n",
    "        -------\n",
    "        reduced: obj\n",
    "            result of reducing self.operation over the specified range of array elements.\n",
    "        \"\"\"\n",
    "        if end is None:\n",
    "            end = self._capacity\n",
    "        if end < 0:\n",
    "            end += self._capacity\n",
    "        end -= 1\n",
    "        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx, val):\n",
    "        # index of the leaf\n",
    "        idx += self._capacity\n",
    "        self._value[idx] = val\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self._value[idx] = self._operation(\n",
    "                self._value[2 * idx],\n",
    "                self._value[2 * idx + 1]\n",
    "            )\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self._capacity\n",
    "        return self._value[self._capacity + idx]\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=operator.add,\n",
    "            neutral_element=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start=0, end=None):\n",
    "        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n",
    "        return super(SumSegmentTree, self).reduce(start, end)\n",
    "\n",
    "    def find_prefixsum_idx(self, prefixsum):\n",
    "        \"\"\"Find the highest index `i` in the array such that\n",
    "            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n",
    "        if array values are probabilities, this function\n",
    "        allows to sample indexes according to the discrete\n",
    "        probability efficiently.\n",
    "        Parameters\n",
    "        ----------\n",
    "        perfixsum: float\n",
    "            upperbound on the sum of array prefix\n",
    "        Returns\n",
    "        -------\n",
    "        idx: int\n",
    "            highest index satisfying the prefixsum constraint\n",
    "        \"\"\"\n",
    "        assert 0 <= prefixsum <= self.sum() + 1e-5\n",
    "        idx = 1\n",
    "        while idx < self._capacity:  # while non-leaf\n",
    "            if self._value[2 * idx] > prefixsum:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                prefixsum -= self._value[2 * idx]\n",
    "                idx = 2 * idx + 1\n",
    "        return idx - self._capacity\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    def __init__(self, capacity):\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity,\n",
    "            operation=min,\n",
    "            neutral_element=float('inf')\n",
    "        )\n",
    "\n",
    "    def min(self, start=0, end=None):\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n",
    "\n",
    "        return super(MinSegmentTree, self).reduce(start, end)\n",
    "# */*/*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "1.0\n",
      "(16, 4)\n",
      "(16,)\n",
      "(16,)\n",
      "(16, 4)\n",
      "(16,)\n",
      "(16,)\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Naive ReplayBuffer\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, \n",
    "                 buffer_size: int, \n",
    "                 input_dim: tuple, \n",
    "                 batch_size: int,\n",
    "                 input_type: str):\n",
    "        \n",
    "        if input_type=='3-dim':\n",
    "            assert len(input_dim)==3, \"The state dimension should be 3-dim! (Channel x Width x Height). Please check if input_dim is right\"\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.save_count, self.current_size = 0, 0\n",
    "\n",
    "        if input_type=='1-dim':\n",
    "            self.state_buffer = np.ones((buffer_size, input_dim), dtype=np.float32) \n",
    "            self.action_buffer = np.ones(buffer_size, dtype=np.uint8) \n",
    "            self.reward_buffer = np.ones(buffer_size, dtype=np.float32) \n",
    "            self.next_state_buffer = np.ones((buffer_size, input_dim), dtype=np.float32)\n",
    "            self.done_buffer = np.ones(buffer_size, dtype=np.uint8) \n",
    "        else:\n",
    "            self.state_buffer = np.ones((buffer_size, input_dim[0], input_dim[1], input_dim[2]), \n",
    "                                        dtype=np.uint8) # WARN: data type is np.int8 so that it should be stored ONLY 0~255 integer!!!\n",
    "            self.action_buffer = np.ones(buffer_size, dtype=np.uint8) \n",
    "            self.reward_buffer = np.ones(buffer_size, dtype=np.float32) \n",
    "            self.next_state_buffer = np.ones((buffer_size, input_dim[0], input_dim[1], input_dim[2]),  \n",
    "                                            dtype=np.uint8) # WARN: data type is np.int8 so that it should be stored ONLY 0~255 integer!!!\n",
    "            self.done_buffer = np.ones(buffer_size, dtype=np.uint8) \n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.current_size\n",
    "\n",
    "    def store(self, \n",
    "              state: np.ndarray, \n",
    "              action: int, \n",
    "              reward: float, \n",
    "              next_state: np.ndarray, \n",
    "              done: int):\n",
    "\n",
    "        self.state_buffer[self.save_count] = state\n",
    "        self.action_buffer[self.save_count] = action\n",
    "        self.reward_buffer[self.save_count] = reward\n",
    "        self.next_state_buffer[self.save_count] = next_state\n",
    "        self.done_buffer[self.save_count] = done\n",
    "        \n",
    "        self.save_count = (self.save_count + 1) % self.buffer_size\n",
    "        self.current_size = min(self.current_size+1, self.buffer_size)\n",
    "\n",
    "    def batch_load(self):\n",
    "        indices = np.random.randint(self.current_size, size=self.batch_size)\n",
    "        return dict(\n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices],\n",
    "                rewards=self.reward_buffer[indices],\n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices]) \n",
    "\n",
    "# */*/*/\n",
    "# ReplayBuffer for Prioritized Experience Replay. \n",
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \n",
    "    def __init__(self, buffer_size, input_dim, batch_size, alpha, input_type):\n",
    "        \n",
    "        super(PrioritizedReplayBuffer, self).__init__(buffer_size, input_dim, batch_size, input_type)\n",
    "        \n",
    "        # For PER. Parameter settings. \n",
    "        self.max_priority, self.tree_idx = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.buffer_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        \n",
    "    def store(self, \n",
    "              state: np.ndarray, \n",
    "              action: int, \n",
    "              reward: float, \n",
    "              next_state: np.ndarray, \n",
    "              done: int):\n",
    "        \n",
    "        super().store(state, action, reward, next_state, done)\n",
    "        \n",
    "        # assigning the maximum priority as an initial value when storing the transitions \n",
    "        self.sum_tree[self.tree_idx] = self.max_priority ** self.alpha\n",
    "        self.min_tree[self.tree_idx] = self.max_priority ** self.alpha\n",
    "        self.tree_idx = (self.tree_idx + 1) % self.buffer_size\n",
    "        \n",
    "    def batch_load(self, beta):\n",
    "        \n",
    "        indices = self._load_batch_indices()\n",
    "\n",
    "        # calculate the maximum weight, which is used to calculate all weights for transitions of the batch\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min*len(self)) ** (-beta)\n",
    "        weights = np.array([self._calculate_weight(idx, beta, max_weight) for idx in indices])\n",
    "        \n",
    "        return dict(\n",
    "                states=self.state_buffer[indices], \n",
    "                actions=self.action_buffer[indices],\n",
    "                rewards=self.reward_buffer[indices],\n",
    "                next_states=self.next_state_buffer[indices], \n",
    "                dones=self.done_buffer[indices],\n",
    "                weights=weights,\n",
    "                indices=indices) \n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # update priorities of transitions after used to update network parameters \n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def _load_batch_indices(self):\n",
    "        \n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self)-1) \n",
    "        segment = p_total / self.batch_size # dividing the sum of all priorities with batch_size to calculate an amount of each segment\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i \n",
    "            b = segment * (i+1)\n",
    "            sample = np.random.uniform(a, b) # sample a value uniformly bewteen two consecutive segments\n",
    "            idx = self.sum_tree.find_prefixsum_idx(sample) # obtaining the index for the sampled value \n",
    "            indices.append(idx)\n",
    "\n",
    "        return indices \n",
    "    \n",
    "    def _calculate_weight(self, idx, beta, max_weight):\n",
    "        \n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample*len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight\n",
    "# */*/*/\n",
    "\n",
    "if __name__=='__main__':\n",
    "    buffer_size = 100\n",
    "    state_dim = 4\n",
    "    batch_size = 16\n",
    "    alpha = 0.6\n",
    "    beta = 0.4\n",
    "    input_type = '1-dim'\n",
    "    buffer = PrioritizedReplayBuffer(buffer_size, state_dim, batch_size, alpha, input_type)\n",
    "    for i in range(50):\n",
    "        state = np.ones(state_dim)\n",
    "        action = 1\n",
    "        reward = 1\n",
    "        next_state = np.ones(state_dim)\n",
    "        done = 1\n",
    "        buffer.store(state, action, reward, next_state, done)\n",
    "    print(buffer.alpha)\n",
    "    print(buffer.max_priority)\n",
    "    print(buffer.batch_load(beta)['states'].shape)\n",
    "    print(buffer.batch_load(beta)['actions'].shape)\n",
    "    print(buffer.batch_load(beta)['rewards'].shape)\n",
    "    print(buffer.batch_load(beta)['next_states'].shape)\n",
    "    print(buffer.batch_load(beta)['dones'].shape)\n",
    "    print(buffer.batch_load(beta)['weights'].shape)\n",
    "    print(buffer.batch_load(beta)['indices'].__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, \n",
    "                 env: 'Environment',\n",
    "                 input_dim: ('int: The width and height of pre-processed input image'),\n",
    "                 training_frames: ('int: The total number of training frames'),\n",
    "                 eps_decay: ('float: Epsilon Decay_rate'),\n",
    "                 gamma: ('float: Discount Factor'),\n",
    "                 target_update_freq: ('int: Target Update Frequency (by frames)'),\n",
    "                 update_type: ('str: Update type for target network. Hard or Soft')='hard',\n",
    "                 soft_update_tau: ('float: Soft update ratio')=None,\n",
    "                 batch_size: ('int: Update batch size')=32,\n",
    "                 buffer_size: ('int: Replay buffer size')=1000000,\n",
    "                 # */*/*/\n",
    "                 alpha: ('int: Hyperparameter for how large prioritization is applied')=0.5,\n",
    "                 beta: ('int: Hyperparameter for the annealing factor of importance sampling')=0.5,\n",
    "                 epsilon_for_priority: ('float: Hyperparameter for adding small increment to the priority')=1e-6, \n",
    "                 # */*/*/\n",
    "                 update_start_buffer_size: ('int: Update starting buffer size')=50000,\n",
    "                 learning_rate: ('float: Learning rate')=0.0004,\n",
    "                 eps_min: ('float: Epsilon Min')=0.1,\n",
    "                 eps_max: ('float: Epsilon Max')=1.0,\n",
    "                 device_num: ('int: GPU device number')=0,\n",
    "                 rand_seed: ('int: Random seed')=None,\n",
    "                 plot_option: ('str: Plotting option')=False,\n",
    "                 model_path: ('str: Model saving path')='./',\n",
    "                 trained_model_path: ('str: Trained model path')=''):\n",
    "\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.device = torch.device(f'cuda:{device_num}' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        self.training_frames = training_frames\n",
    "        self.epsilon = eps_max\n",
    "        self.eps_decay = eps_decay\n",
    "        self.eps_min = eps_min\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.update_cnt = 0\n",
    "        self.update_type = update_type\n",
    "        self.tau = soft_update_tau\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.update_start = update_start_buffer_size\n",
    "        self.seed = rand_seed\n",
    "        self.plot_option = plot_option\n",
    "        \n",
    "        # */*/*/\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_step = (1.0 - beta) / self.training_frames\n",
    "        self.epsilon_for_priority = epsilon_for_priority\n",
    "        # */*/*/\n",
    "        \n",
    "        self.q_behave = QNetwork(self.input_dim, self.action_dim).to(self.device)\n",
    "        self.q_target = QNetwork(self.input_dim, self.action_dim).to(self.device)\n",
    "        if trained_model_path: # load a trained model if existing\n",
    "            self.q_behave.load_state_dict(torch.load(trained_model_path))\n",
    "            print(\"Trained model is loaded successfully.\")\n",
    "        \n",
    "        # Initialize target network parameters with behavior network parameters\n",
    "        self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "        self.q_target.eval()\n",
    "        self.optimizer = optim.Adam(self.q_behave.parameters(), lr=learning_rate) \n",
    "\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.input_dim, self.batch_size)\n",
    "\n",
    "    def select_action(self, state: 'Must be pre-processed in the same way as updating current Q network. See def _compute_loss'):\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.zeros(self.action_dim), self.env.action_space.sample()\n",
    "        else:\n",
    "            # with no_grad to compute faster\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                Qs = self.q_behave(state)\n",
    "                # take an action of a maximum Q-value\n",
    "                action = Qs.argmax()\n",
    "            \n",
    "            # return action and Q-values (Q-values are not required for implementing algorithms. This is just for checking Q-values for each state. Not must-needed)  \n",
    "            return Qs.detach().cpu().numpy(), action.detach().item()  \n",
    "\n",
    "    def get_init_state(self):\n",
    "\n",
    "        init_state = self.env.reset()\n",
    "        for _ in range(0): # loop for a random initial starting point. range(0) means the same initial point.\n",
    "            action = self.env.action_space.sample()\n",
    "            init_state, _, _, _ = self.env.step(action) \n",
    "        return init_state\n",
    "\n",
    "    def get_state(self, state, action):\n",
    "\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        return reward, next_state, done\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.memory.store(state, action, reward, next_state, done)\n",
    "\n",
    "    def update_behavior_q_net(self):\n",
    "        # update behavior q network with a batch\n",
    "        batch = self.memory.batch_load()\n",
    "        loss = self._compute_loss(batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def target_soft_update(self):\n",
    "        ''' target network is updated with Soft Update. tau is a hyperparameter for the updating ratio betweeen target and behavior network  '''\n",
    "        for target_param, current_param in zip(self.q_target.parameters(), self.q_behave.parameters()):\n",
    "            target_param.data.copy_(self.tau*current_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "    def target_hard_update(self):\n",
    "        ''' target network is updated with Hard Update '''\n",
    "        self.update_cnt = (self.update_cnt+1) % self.target_update_freq\n",
    "        if self.update_cnt==0:\n",
    "            self.q_target.load_state_dict(self.q_behave.state_dict())\n",
    "\n",
    "    def train(self):\n",
    "        tic = time.time()\n",
    "        losses = []\n",
    "        scores = []\n",
    "        epsilons = []\n",
    "        avg_scores = [[-10000]] # As an initial score, set an arbitrary score of an episode.\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        print(\"Storing initial buffer..\") \n",
    "        state = self.get_init_state()\n",
    "        for frame_idx in range(1, self.update_start+1):\n",
    "            # Store transitions into the buffer until the number of 'self.update_start' transitions is stored \n",
    "            _, action = self.select_action(state)\n",
    "            reward, next_state, done = self.get_state(state, action)\n",
    "            self.store(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done: state = self.get_init_state()\n",
    "\n",
    "        print(\"Done. Start learning..\")\n",
    "        history_store = []\n",
    "        for frame_idx in range(1, self.training_frames+1):\n",
    "            Qs, action = self.select_action(state)\n",
    "            reward, next_state, done = self.get_state(state, action)\n",
    "            self.store(state, action, reward, next_state, done)\n",
    "            history_store.append([state, Qs, action, reward, next_state, done]) # history_store is for checking an episode later. Not must-needed.\n",
    "            loss = self.update_behavior_q_net()\n",
    "\n",
    "            if self.update_type=='hard':   self.target_hard_update()\n",
    "            elif self.update_type=='soft': self.target_soft_update()\n",
    "            \n",
    "            score += reward\n",
    "            losses.append(loss)\n",
    "\n",
    "            if done:\n",
    "                # For saving and plotting when an episode is done.\n",
    "                scores.append(score)\n",
    "                if np.mean(scores[-10:]) > max(avg_scores):\n",
    "                    torch.save(self.q_behave.state_dict(), self.model_path+'{}_Score:{}.pt'.format(frame_idx, np.mean(scores[-10:])))\n",
    "                    training_time = round((time.time()-tic)/3600, 1)\n",
    "                    np.save(self.model_path+'{}_history_Score_{}_{}hrs.npy'.format(frame_idx, score, training_time), np.array(history_store))\n",
    "                    print(\"          | Model saved. Recent scores: {}, Training time: {}hrs\".format(scores[-10:], training_time), ' /'.join(os.getcwd().split('/')[-3:]))\n",
    "                avg_scores.append(np.mean(scores[-10:]))\n",
    "\n",
    "                if self.plot_option=='inline': \n",
    "                    scores.append(score)\n",
    "                    epsilons.append(self.epsilon)\n",
    "                    self._plot(frame_idx, scores, losses, epsilons)\n",
    "                else: \n",
    "                    print(score, end='\\r')\n",
    "\n",
    "                score=0\n",
    "                state = self.get_init_state()\n",
    "                history_store = []\n",
    "            else: state = next_state\n",
    "\n",
    "            self._epsilon_step()\n",
    "\n",
    "        print(\"Total training time: {}(hrs)\".format((time.time()-tic)/3600))\n",
    "\n",
    "    def _epsilon_step(self):\n",
    "        self.epsilon = max(self.epsilon-self.eps_decay, 0.1)\n",
    "\n",
    "    def _compute_loss(self, batch: \"Dictionary (S, A, R', S', Dones)\"):\n",
    "        states = torch.FloatTensor(batch['states']).to(self.device)\n",
    "        next_states = torch.FloatTensor(batch['next_states']).to(self.device)\n",
    "        actions = torch.LongTensor(batch['actions'].reshape(-1, 1)).to(self.device)\n",
    "        rewards = torch.FloatTensor(batch['rewards'].reshape(-1, 1)).to(self.device)\n",
    "        dones = torch.FloatTensor(batch['dones'].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        current_q = self.q_behave(states).gather(1, actions)\n",
    "\n",
    "        # target value\n",
    "        next_q = self.q_target(next_states).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - dones\n",
    "        target = (rewards + (mask * self.gamma * next_q)).to(self.device)\n",
    "\n",
    "        # Use smooth l1 loss for clipping loss between -1 to 1 as in DQN paper.\n",
    "        loss = F.smooth_l1_loss(current_q, target)\n",
    "        return loss\n",
    "\n",
    "    def _plot(self, frame_idx, scores, losses, epsilons):\n",
    "        clear_output(True) \n",
    "        plt.figure(figsize=(20, 5), facecolor='w') \n",
    "        plt.subplot(131)  \n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores) \n",
    "        plt.subplot(132) \n",
    "        plt.title('loss') \n",
    "        plt.plot(losses) \n",
    "        plt.subplot(133) \n",
    "        plt.title('epsilons')\n",
    "        plt.plot(epsilons) \n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurations\n",
    "\n",
    "![image](https://drive.google.com/uc?id=1P_PgreL2VnMTFTQAbM1wxANka8pu7bx0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env_name CartPole-v0\n",
      "model_save_path: ./model_save/Test/\n"
     ]
    }
   ],
   "source": [
    "env_list = {\n",
    "    0: \"CartPole-v0\",\n",
    "    1: \"CartPole-v2\",\n",
    "    2: \"LunarLander-v2\",\n",
    "}\n",
    "\n",
    "env_name = env_list[0]\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Same input size as in DQN paper. \n",
    "input_dim = env.observation_space.shape[0]\n",
    "print(\"env_name\", env_name) \n",
    "update_start_buffer_size = 200\n",
    "training_frames = 20000\n",
    "eps_max = 1.0\n",
    "eps_min = 0.1\n",
    "eps_decay = 1/2000\n",
    "gamma = 0.99\n",
    "\n",
    "buffer_size = int(2e3) \n",
    "batch_size = 32           \n",
    "update_type = 'hard'\n",
    "soft_update_tau = 0.002\n",
    "learning_rate = 0.001\n",
    "target_update_freq = 100\n",
    "\n",
    "device_num = 0\n",
    "rand_seed = None\n",
    "rand_name = ('').join(map(str, np.random.randint(10, size=(3,))))\n",
    "folder_name = os.getcwd().split('/')[-1] \n",
    "\n",
    "model_name = 'Test'\n",
    "model_save_path = f'./model_save/{model_name}/'\n",
    "if not os.path.exists('./model_save/'):\n",
    "    os.mkdir('./model_save/')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)\n",
    "print(\"model_save_path:\", model_save_path)\n",
    "\n",
    "trained_model_path = ''\n",
    "\n",
    "#for PER\n",
    "alpha = 0.5\n",
    "beta = 0.6\n",
    "epsilon_for_priority = 1e-6\n",
    "\n",
    "plot_options = {1: 'inline', 2: False} \n",
    "plot_option = plot_options[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = Agent( \n",
    "    env,\n",
    "    input_dim,\n",
    "    training_frames,\n",
    "    eps_decay,\n",
    "    gamma,\n",
    "    target_update_freq,\n",
    "    update_type,\n",
    "    soft_update_tau,\n",
    "    batch_size,\n",
    "    buffer_size,\n",
    "    update_start_buffer_size,\n",
    "    learning_rate,\n",
    "    eps_min,\n",
    "    eps_max,\n",
    "    device_num,\n",
    "    rand_seed,\n",
    "    plot_option,\n",
    "    model_save_path,\n",
    "    trained_model_path\n",
    ") \n",
    "\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An example of results\n",
    "\n",
    "    Storing initial buffer..\n",
    "    Done. Start learning..\n",
    "              | Model saved. Recent scores: [31.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [20.0, 30.0, 16.0, 42.0, 49.0, 22.0, 23.0, 24.0, 13.0, 84.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [30.0, 16.0, 42.0, 49.0, 22.0, 23.0, 24.0, 13.0, 84.0, 50.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [42.0, 49.0, 22.0, 23.0, 24.0, 13.0, 84.0, 50.0, 24.0, 126.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [84.0, 50.0, 24.0, 126.0, 13.0, 22.0, 35.0, 21.0, 49.0, 76.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [126.0, 13.0, 22.0, 35.0, 21.0, 49.0, 76.0, 33.0, 95.0, 145.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [13.0, 22.0, 35.0, 21.0, 49.0, 76.0, 33.0, 95.0, 145.0, 145.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [22.0, 35.0, 21.0, 49.0, 76.0, 33.0, 95.0, 145.0, 145.0, 126.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [35.0, 21.0, 49.0, 76.0, 33.0, 95.0, 145.0, 145.0, 126.0, 112.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [21.0, 49.0, 76.0, 33.0, 95.0, 145.0, 145.0, 126.0, 112.0, 161.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [49.0, 76.0, 33.0, 95.0, 145.0, 145.0, 126.0, 112.0, 161.0, 193.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [76.0, 33.0, 95.0, 145.0, 145.0, 126.0, 112.0, 161.0, 193.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [33.0, 95.0, 145.0, 145.0, 126.0, 112.0, 161.0, 193.0, 200.0, 175.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [95.0, 145.0, 145.0, 126.0, 112.0, 161.0, 193.0, 200.0, 175.0, 139.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [145.0, 145.0, 126.0, 112.0, 161.0, 193.0, 200.0, 175.0, 139.0, 150.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [145.0, 126.0, 112.0, 161.0, 193.0, 200.0, 175.0, 139.0, 150.0, 168.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [126.0, 112.0, 161.0, 193.0, 200.0, 175.0, 139.0, 150.0, 168.0, 187.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [112.0, 161.0, 193.0, 200.0, 175.0, 139.0, 150.0, 168.0, 187.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [161.0, 193.0, 200.0, 175.0, 139.0, 150.0, 168.0, 187.0, 200.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [193.0, 200.0, 175.0, 139.0, 150.0, 168.0, 187.0, 200.0, 200.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [200.0, 175.0, 139.0, 150.0, 168.0, 187.0, 200.0, 200.0, 200.0, 195.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [150.0, 168.0, 187.0, 200.0, 200.0, 200.0, 195.0, 155.0, 200.0, 178.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [168.0, 187.0, 200.0, 200.0, 200.0, 195.0, 155.0, 200.0, 178.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [187.0, 200.0, 200.0, 200.0, 195.0, 155.0, 200.0, 178.0, 200.0, 169.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [200.0, 200.0, 200.0, 195.0, 155.0, 200.0, 178.0, 200.0, 169.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN\n",
    "              | Model saved. Recent scores: [200.0, 183.0, 192.0, 200.0, 200.0, 157.0, 176.0, 200.0, 193.0, 200.0], Training time: 0.0hrs MacaronRL /Value_Based /Vanila_DQN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
